{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b4247a9f",
      "metadata": {},
      "source": [
        "# Assignment 2\n",
        "\n",
        "Imports and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5106b3b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "472258bb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((49000, 784), (21000, 784))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "assert x_train.shape == (60000, 28, 28)\n",
        "assert x_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "# Combine so we can split later \n",
        "x_combined = np.concatenate([x_train, x_test], axis=0)\n",
        "y_combined = np.concatenate([y_train, y_test], axis=0)\n",
        "\n",
        "# Normalize\n",
        "x_combined = x_combined.astype('float32') / 255.0\n",
        "\n",
        "# Flatten images to vectors\n",
        "x_combined = x_combined.reshape(-1, 28*28)\n",
        "\n",
        "# 70/30 split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_combined, y_combined, test_size=0.30, random_state=7, stratify=y_combined\n",
        ")\n",
        "\n",
        "x_train.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "630ce3c2",
      "metadata": {},
      "source": [
        "## Task 1: Compare 2-layer vs 3-layer vs 4-layer MLPs\n",
        "\n",
        "### All models use 100 units per hidden layer with ReLU activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "58c5d968",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2-layer | train acc: 0.9997 | test acc: 0.9750\n",
            "3-layer | train acc: 1.0000 | test acc: 0.9771\n",
            "4-layer | train acc: 0.9948 | test acc: 0.9713\n"
          ]
        }
      ],
      "source": [
        "# Compare depth: 2-layer vs 3-layer vs 4-layer (hidden layers)\n",
        "\n",
        "# A lot of bad code reuse here but its okay for this assignment \n",
        "\n",
        "# 2-layer MLP\n",
        "model_2 = keras.Sequential([\n",
        "    layers.Input(shape=(784,)),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model_2.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=2e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model_2.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=0\n",
        ")\n",
        "train_loss_2, train_acc_2 = model_2.evaluate(x_train, y_train, verbose=0)\n",
        "test_loss_2, test_acc_2 = model_2.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# 3-layer MLP\n",
        "model_3 = keras.Sequential([\n",
        "    layers.Input(shape=(784,)),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model_3.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=2e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model_3.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=0\n",
        ")\n",
        "train_loss_3, train_acc_3 = model_3.evaluate(x_train, y_train, verbose=0)\n",
        "test_loss_3, test_acc_3 = model_3.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# 4-layer MLP\n",
        "model_4 = keras.Sequential([\n",
        "    layers.Input(shape=(784,)),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model_4.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=2e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model_4.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=0\n",
        ")\n",
        "train_loss_4, train_acc_4 = model_4.evaluate(x_train, y_train, verbose=0)\n",
        "test_loss_4, test_acc_4 = model_4.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"2-layer | train acc: {train_acc_2:.4f} | test acc: {test_acc_2:.4f}\")\n",
        "print(f\"3-layer | train acc: {train_acc_3:.4f} | test acc: {test_acc_3:.4f}\")\n",
        "print(f\"4-layer | train acc: {train_acc_4:.4f} | test acc: {test_acc_4:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba6d727",
      "metadata": {},
      "source": [
        "### 2 vs 3 vs 4 layer MLP comparison\n",
        "\n",
        "It seems that overall the 3-layer MLP performs the best on the MNIST dataset. The 4-layer has slightly degraded performance compared to the 3-layer, likely because this is a relatively simple task and 4-layers is trying to do too much. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff80774e",
      "metadata": {},
      "source": [
        "## Task 2: Compare different 2-layer MLP Variants\n",
        "\n",
        "- Weight initializations\n",
        "- Regularizations\n",
        "- Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1d9dc471",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "glorot_uniform  | train acc: 1.0000 | test acc: 0.9760\n",
            "he_normal       | train acc: 1.0000 | test acc: 0.9784\n"
          ]
        }
      ],
      "source": [
        "# Weight initialization comparison on 2-layer MLP\n",
        "init_methods = [\"glorot_uniform\", \"he_normal\"]\n",
        "init_results = []\n",
        "\n",
        "for init in init_methods:\n",
        "    # 2-layer MLP\n",
        "    model_weights_init = keras.Sequential([\n",
        "        layers.Input(shape=(784,)),\n",
        "        layers.Dense(100, activation='relu', kernel_initializer=init),\n",
        "        layers.Dense(100, activation='relu', kernel_initializer=init),\n",
        "        layers.Dense(10, activation='softmax', kernel_initializer=init)\n",
        "    ])\n",
        "    model_weights_init.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=2e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model_weights_init.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_data=(x_test, y_test),\n",
        "        verbose=0\n",
        "    )\n",
        "    train_loss_init, train_acc_init = model_weights_init.evaluate(x_train, y_train, verbose=0)\n",
        "    test_loss_init, test_acc_init = model_weights_init.evaluate(x_test, y_test, verbose=0)\n",
        "    init_results.append((init, train_acc_init, test_acc_init))\n",
        "\n",
        "for init, train_acc, test_acc in init_results:\n",
        "    print(f\"{init:15s} | train acc: {train_acc:.4f} | test acc: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5d53bc5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L2 (1e-4)       | train acc: 0.9992 | test acc: 0.9770\n",
            "L1 (1e-4)       | train acc: 0.9926 | test acc: 0.9733\n"
          ]
        }
      ],
      "source": [
        "# Regularization comparison on 2-layer MLP\n",
        "# Using dict for printing purposes\n",
        "reg_methods = {\n",
        "    \"L2 (1e-4)\": keras.regularizers.l2(1e-4),\n",
        "    \"L1 (1e-4)\": keras.regularizers.l1(1e-4),\n",
        "}\n",
        "reg_results = []\n",
        "\n",
        "for reg_name, reg in reg_methods.items():\n",
        "    # 2-layer MLP\n",
        "    model_reg = keras.Sequential([\n",
        "        layers.Input(shape=(784,)),\n",
        "        layers.Dense(100, activation='relu', kernel_regularizer=reg),\n",
        "        layers.Dense(100, activation='relu', kernel_regularizer=reg),\n",
        "        layers.Dense(10, activation='softmax', kernel_regularizer=reg)\n",
        "    ])\n",
        "    model_reg.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=2e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model_reg.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_data=(x_test, y_test),\n",
        "        verbose=0\n",
        "    )\n",
        "    train_loss_reg, train_acc_reg = model_reg.evaluate(x_train, y_train, verbose=0)\n",
        "    test_loss_reg, test_acc_reg = model_reg.evaluate(x_test, y_test, verbose=0)\n",
        "    reg_results.append((reg_name, train_acc_reg, test_acc_reg))\n",
        "\n",
        "for reg_name, train_acc, test_acc in reg_results:\n",
        "    print(f\"{reg_name:15s} | train acc: {train_acc:.4f} | test acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "79a077fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SGD             | train acc: 0.8737 | test acc: 0.8687\n",
            "Adam            | train acc: 1.0000 | test acc: 0.9761\n"
          ]
        }
      ],
      "source": [
        "# Optimizer comparison on 2-layer MLP\n",
        "opt_methods = {\n",
        "    \"SGD\": keras.optimizers.SGD(learning_rate=0.0001),\n",
        "    \"Adam\": keras.optimizers.Adam(learning_rate=2e-4),\n",
        "}\n",
        "opt_results = []\n",
        "\n",
        "for opt_name, opt in opt_methods.items():\n",
        "    # 2-layer MLP\n",
        "    model_opt = keras.Sequential([\n",
        "        layers.Input(shape=(784,)),\n",
        "        layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model_opt.compile(\n",
        "        optimizer=opt,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model_opt.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_data=(x_test, y_test),\n",
        "        verbose=0\n",
        "    )\n",
        "    train_loss_opt, train_acc_opt = model_opt.evaluate(x_train, y_train, verbose=0)\n",
        "    test_loss_opt, test_acc_opt = model_opt.evaluate(x_test, y_test, verbose=0)\n",
        "    opt_results.append((opt_name, train_acc_opt, test_acc_opt))\n",
        "\n",
        "for opt_name, train_acc, test_acc in opt_results:\n",
        "    print(f\"{opt_name:15s} | train acc: {train_acc:.4f} | test acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd9fcdf",
      "metadata": {},
      "source": [
        "### Final summary\n",
        "\n",
        "- Weight initializations\n",
        "    - glorot_uniform  | train acc: 1.0000 | test acc: 0.9760\n",
        "    - he_normal       | train acc: 1.0000 | test acc: 0.9784\n",
        "- Regularizations\n",
        "    - L2 (1e-4)       | train acc: 0.9992 | test acc: 0.9770\n",
        "    - L1 (1e-4)       | train acc: 0.9926 | test acc: 0.9733\n",
        "- Optimizers\n",
        "    - SGD             | train acc: 0.8737 | test acc: 0.8687\n",
        "    - Adam            | train acc: 1.0000 | test acc: 0.9761\n",
        "\n",
        "The only real difference between these is the Adam optimizer when compared to SGD. Adam achieved a perfect training accuracy and substantially higher test accuracy compared to SGD showing faster convergence and overall better optimization for this task which was seen for all the other variants, that also used Adam."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
