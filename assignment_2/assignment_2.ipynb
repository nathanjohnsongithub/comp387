{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b4247a9f",
      "metadata": {},
      "source": [
        "# Assignment 2\n",
        "\n",
        "Imports and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5106b3b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "472258bb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((60000, 784), (10000, 784))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Verify it loaded correctly\n",
        "assert x_train.shape == (60000, 28, 28)\n",
        "assert x_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "# Normalize to [0,1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Flatten\n",
        "x_train = x_train.reshape((x_train.shape[0], -1))\n",
        "x_test = x_test.reshape((x_test.shape[0], -1))\n",
        "\n",
        "x_train.shape, x_test.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "630ce3c2",
      "metadata": {},
      "source": [
        "## Task 1: Compare 2-layer vs 3-layer vs 4-layer MLPs\n",
        "\n",
        "### All models use 100 units per hidden layer with ReLU activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "58c5d968",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2-layer | train acc: 0.8830 | test acc: 0.8886\n",
            "3-layer | train acc: 0.8899 | test acc: 0.8961\n",
            "4-layer | train acc: 0.8814 | test acc: 0.8865\n"
          ]
        }
      ],
      "source": [
        "# Compare depth: 2-layer vs 3-layer vs 4-layer (hidden layers)\n",
        "# 2-layer MLP\n",
        "model_2 = keras.Sequential([\n",
        "    layers.Input(shape=(784,)),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model_2.compile(\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model_2.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=0\n",
        ")\n",
        "train_loss_2, train_acc_2 = model_2.evaluate(x_train, y_train, verbose=0)\n",
        "test_loss_2, test_acc_2 = model_2.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# 3-layer MLP\n",
        "model_3 = keras.Sequential([\n",
        "    layers.Input(shape=(784,)),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model_3.compile(\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model_3.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=0\n",
        ")\n",
        "train_loss_3, train_acc_3 = model_3.evaluate(x_train, y_train, verbose=0)\n",
        "test_loss_3, test_acc_3 = model_3.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# 4-layer MLP\n",
        "model_4 = keras.Sequential([\n",
        "    layers.Input(shape=(784,)),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model_4.compile(\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model_4.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=0\n",
        ")\n",
        "train_loss_4, train_acc_4 = model_4.evaluate(x_train, y_train, verbose=0)\n",
        "test_loss_4, test_acc_4 = model_4.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"2-layer | train acc: {train_acc_2:.4f} | test acc: {test_acc_2:.4f}\")\n",
        "print(f\"3-layer | train acc: {train_acc_3:.4f} | test acc: {test_acc_3:.4f}\")\n",
        "print(f\"4-layer | train acc: {train_acc_4:.4f} | test acc: {test_acc_4:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba6d727",
      "metadata": {},
      "source": [
        "### 2 vs 3 vs 4 layer MLP comparison\n",
        "\n",
        "It seems that overall the 3-layer MLP performs the best on the MNIST dataset. The 4-layer has slightly degraded performance compared to the 3-layer, likely because this is a relatively simple task and 4-layers is trying to do too much. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff80774e",
      "metadata": {},
      "source": [
        "## Task 2: Compare different 2-layer MLP Variants\n",
        "\n",
        "- Weight initializations\n",
        "- Regularizations\n",
        "- Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d9dc471",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "glorot_uniform  | train acc: 0.8825 | test acc: 0.8882\n",
            "he_normal       | train acc: 0.8906 | test acc: 0.8968\n"
          ]
        }
      ],
      "source": [
        "# Weight initialization comparison on 2-layer MLP\n",
        "init_methods = [\"glorot_uniform\", \"he_normal\"]\n",
        "init_results = []\n",
        "\n",
        "for init in init_methods:\n",
        "    # 2-layer MLP\n",
        "    model_weights_init = keras.Sequential([\n",
        "        layers.Input(shape=(784,)),\n",
        "        layers.Dense(100, activation='relu', kernel_initializer=init),\n",
        "        layers.Dense(100, activation='relu', kernel_initializer=init),\n",
        "        layers.Dense(10, activation='softmax', kernel_initializer=init)\n",
        "    ])\n",
        "    model_weights_init.compile(\n",
        "        optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model_weights_init.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_data=(x_test, y_test),\n",
        "        verbose=0\n",
        "    )\n",
        "    train_loss_init, train_acc_init = model_weights_init.evaluate(x_train, y_train, verbose=0)\n",
        "    test_loss_init, test_acc_init = model_weights_init.evaluate(x_test, y_test, verbose=0)\n",
        "    init_results.append((init, train_acc_init, test_acc_init))\n",
        "\n",
        "for init, train_acc, test_acc in init_results:\n",
        "    print(f\"{init:15s} | train acc: {train_acc:.4f} | test acc: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5d53bc5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L2 (1e-4)       | train acc: 0.8865 | test acc: 0.8896\n",
            "L1 (1e-4)       | train acc: 0.8859 | test acc: 0.8926\n"
          ]
        }
      ],
      "source": [
        "# Regularization comparison on 2-layer MLP\n",
        "# Using dict for printing purposes\n",
        "reg_methods = {\n",
        "    \"L2 (1e-4)\": keras.regularizers.l2(1e-4),\n",
        "    \"L1 (1e-4)\": keras.regularizers.l1(1e-4),\n",
        "}\n",
        "reg_results = []\n",
        "\n",
        "for reg_name, reg in reg_methods.items():\n",
        "    # 2-layer MLP\n",
        "    model_reg = keras.Sequential([\n",
        "        layers.Input(shape=(784,)),\n",
        "        layers.Dense(100, activation='relu', kernel_regularizer=reg),\n",
        "        layers.Dense(100, activation='relu', kernel_regularizer=reg),\n",
        "        layers.Dense(10, activation='softmax', kernel_regularizer=reg)\n",
        "    ])\n",
        "    model_reg.compile(\n",
        "        optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model_reg.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_data=(x_test, y_test),\n",
        "        verbose=0\n",
        "    )\n",
        "    train_loss_reg, train_acc_reg = model_reg.evaluate(x_train, y_train, verbose=0)\n",
        "    test_loss_reg, test_acc_reg = model_reg.evaluate(x_test, y_test, verbose=0)\n",
        "    reg_results.append((reg_name, train_acc_reg, test_acc_reg))\n",
        "\n",
        "for reg_name, train_acc, test_acc in reg_results:\n",
        "    print(f\"{reg_name:15s} | train acc: {train_acc:.4f} | test acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "79a077fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SGD             | train acc: 0.8821 | test acc: 0.8875\n",
            "Adam            | train acc: 0.9996 | test acc: 0.9780\n"
          ]
        }
      ],
      "source": [
        "# Optimizer comparison on 2-layer MLP\n",
        "opt_methods = {\n",
        "    \"SGD\": keras.optimizers.SGD(learning_rate=0.0001),\n",
        "    \"Adam\": keras.optimizers.Adam(learning_rate=0.0001),\n",
        "}\n",
        "opt_results = []\n",
        "\n",
        "for opt_name, opt in opt_methods.items():\n",
        "    # 2-layer MLP\n",
        "    model_opt = keras.Sequential([\n",
        "        layers.Input(shape=(784,)),\n",
        "        layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model_opt.compile(\n",
        "        optimizer=opt,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model_opt.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_data=(x_test, y_test),\n",
        "        verbose=0\n",
        "    )\n",
        "    train_loss_opt, train_acc_opt = model_opt.evaluate(x_train, y_train, verbose=0)\n",
        "    test_loss_opt, test_acc_opt = model_opt.evaluate(x_test, y_test, verbose=0)\n",
        "    opt_results.append((opt_name, train_acc_opt, test_acc_opt))\n",
        "\n",
        "for opt_name, train_acc, test_acc in opt_results:\n",
        "    print(f\"{opt_name:15s} | train acc: {train_acc:.4f} | test acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd9fcdf",
      "metadata": {},
      "source": [
        "### Final summary\n",
        "\n",
        "- Weight initializations\n",
        "    - glorot_uniform  | train acc: 0.8825 | test acc: 0.8882\n",
        "    - he_normal       | train acc: 0.8906 | test acc: 0.8968\n",
        "- Regularizations\n",
        "    - L2 (1e-4)       | train acc: 0.8865 | test acc: 0.8896\n",
        "    - L1 (1e-4)       | train acc: 0.8859 | test acc: 0.8926\n",
        "- Optimizers\n",
        "    - SGD             | train acc: 0.8821 | test acc: 0.8875\n",
        "    - Adam            | train acc: 0.9996 | test acc: 0.9780\n",
        "\n",
        "The only real difference between these is the Adam optimizer. Adam achieved a near-perfect training accuracy and substantially higher test accuracy compared to SGD showing faster convergence and overall better optimization for this task."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
